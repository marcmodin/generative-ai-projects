{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Simple Local RAG\n",
    "\n",
    "This is a very exploration with Langchain, Langsmith and AWS Bedrock, on how to create basic RAG with a locally running ChromaDB instance loaded with a single source PDF document.\n",
    "\n",
    "Langsmith is not actually needed for the implementation but its useful if we want to trace whats going on in our application.\n",
    "\n",
    "### Prerequisites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt --quiet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate to AWS and Langsmith and Load Envrionment Variables\n",
    "Follow the instuctions in the `README.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Environment Variables\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv())  # load the environment variables from .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Bedrock\n",
    "creating a bedrock client which will be used by the Bedrock classes in Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "# Get region and profile from env\n",
    "region = os.environ.get(\"AWS_REGION\", \"us-east-1\")\n",
    "\n",
    "# Create a Bedrock client\n",
    "bedrock_client = boto3.client(service_name='bedrock-runtime', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Langsmith Client\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Documents (Lost in the Middle)\n",
    "\n",
    "Download [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/pdf/2307.03172.pdf) pdf paper, then split, chunk and embed the entire document into ChromaDB \n",
    "\n",
    "To test out the LLM have a look at this paper to get an idea of what questions you can ask \n",
    "or load in another online pdf from somewhere else.\n",
    "\n",
    "Remember that you only have to load this in the first time you run this notebook to embed the data\n",
    "into the vector-store. For subsequent any runs skip this step.\n",
    "\n",
    "**This step should be skipped if you have already created the vector-store.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "url = \"https://arxiv.org/pdf/2307.03172.pdf\"\n",
    "loader = PyPDFLoader(url)\n",
    "data = loader.load()\n",
    "\n",
    "print(len(data))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the pdf pages loaded we then need prepare the data by splitting the text into smaller chunks using \"fixed sized chunking\" method. \n",
    "\n",
    "> Depending on the text embedding model you will be using, you may need to take into account the model's max-context window and max-embedding-dimensions. \n",
    "\n",
    "**This step should also be skipped if you have already created the vector-store.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 128)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "len(all_splits)\n",
    "\n",
    "# you can iterate of the first 10 splits and print out each split to check your chunking is working as expected\n",
    "for split in all_splits[:10]:\n",
    "    print(split)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the embeddings from the pdf document with Bedrock\n",
    "Creating vector embeddings from documents involves converting textual information into numerical form so that it can be processed and understood by machine learning models.\n",
    "\n",
    "The embedding model is used for both creating the embeddings of pdf documents which will be stored in the vector-store as well as for creating the embedding of the user question.\n",
    "\n",
    "Langchain has built in support for many different embedding models. https://python.langchain.com/docs/integrations/text_embedding/bedrock\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BedrockEmbeddings Model\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "embedding_model_id = \"amazon.titan-embed-text-v1\"\n",
    "\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=bedrock_client,\n",
    "    model_id=embedding_model_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing ChromaDB and generating the embeddings\n",
    "\n",
    "**The following step should also be skipped if you have already created the vector-store.**\n",
    "\n",
    "> Note: if you need to recreate the embeddings, the easiest is to simply delete the `./chroma_db` directory to start from scratch!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Embeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits, embedding=embedding_model, persist_directory='chroma_db')\n",
    "\n",
    "vectorstore.persist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your local ChromaDB vector store as a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# load from disk\n",
    "# Note: The following code is demonstrating how to load the Chroma database from disk.\n",
    "vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_model)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# testing the retriever by similarity search\n",
    "retriever.get_relevant_documents(\"why are language models not robust to changes in the position of relevant information?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain Hub\n",
    "\n",
    "The hub is which also a part of Langsmith is a place to discover, share, and version control prompts and either share it with the community or keep it for private use.\n",
    "\n",
    "The following will import a predefined prompt-template that will be used in the retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling in a forked community prompt\n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"mrkmod/rag-prompt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Retrieval Augmentented Generation chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Bedrock\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# experiment with different models if you like\n",
    "llm_model_id = \"anthropic.claude-instant-v1\"\n",
    "# llm_model_id = \"anthropic.claude-v2\"\n",
    "\n",
    "llm = Bedrock(\n",
    "    client=bedrock_client, model_id=llm_model_id\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Retrieval Chain\n",
    "\n",
    "question = \"why are language models not robust to changes in the position of relevant information?\"\n",
    "\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "\n",
    "print(result[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
